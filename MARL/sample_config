{'extra_python_environs_for_driver': {}, 
'extra_python_environs_for_worker': {}, 
'num_gpus': 1, 
'num_cpus_per_worker': 1, 
'num_gpus_per_worker': 0, 
'_fake_gpus': False, 
'custom_resources_per_worker': {}, 
'placement_strategy': 'PACK', 
'eager_tracing': False, 
'eager_max_retraces': 20, 
'tf_session_args': {
	'intra_op_parallelism_threads': 2, 
	'inter_op_parallelism_threads': 2, 
	'gpu_options': {'allow_growth': True}, 
	'log_device_placement': False, 
	'device_count': {'CPU': 1}, 
	'allow_soft_placement': True}, 
'local_tf_session_args': {
	'intra_op_parallelism_threads': 8, 
	'inter_op_parallelism_threads': 8}, 
'env': 'meltingpot', 
'env_config': {
	'substrate': 'bach_or_stravinsky_in_the_matrix__repeated', 
	'roles': ['bach_fan', 'stravinsky_fan']}, 
	'observation_space': None, 
	'action_space': None, 
	'env_task_fn': None, 
	'render_env': False, 
	'clip_rewards': None, 
	'normalize_actions': True, 
	'clip_actions': False, 
	'disable_env_checking': False, 
	'num_workers': 2, 
	'num_envs_per_worker': 1, 
	'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 
	'sample_async': False, 
	'enable_connectors': False, 
	'rollout_fragment_length': 100, 
	'batch_mode': 'truncate_episodes', 
	'remote_worker_envs': False, 
	'remote_env_batch_wait_ms': 0, 
	'validate_workers_after_construction': True, 
	'ignore_worker_failures': False, 
	'recreate_failed_workers': False, 
	'restart_failed_sub_environments': False, 
	'num_consecutive_worker_failures_tolerance': 100, 
	'horizon': None, 
	'soft_horizon': False, 
	'no_done_at_end': False, 
	'preprocessor_pref': None, 
	'observation_filter': 'NoFilter', 
	'synchronize_filters': True, 
	'compress_observations': False, 
	'enable_tf1_exec_eagerly': False, 
	'sampler_perf_stats_ema_coef': None, 
	'gamma': 0.99, 
	'lr': 5e-05, 
	'train_batch_size': 6400, 
	'model': {
		'_use_default_native_models': False, 
		'_disable_preprocessor_api': False, 
		'_disable_action_flattening': False, 
		'fcnet_hiddens': [64, 64], 
		'fcnet_activation': 'relu', 
		'conv_filters': None, 
		'conv_activation': 'relu', 
		'post_fcnet_hiddens': [256], 
		'post_fcnet_activation': 'relu', 
		'free_log_std': False, 
		'no_final_linear': False, 
		'vf_share_layers': False, 
		'use_lstm': True, 
		'max_seq_len': 20, 
		'lstm_cell_size': 256, 
		'lstm_use_prev_action': True, 
		'lstm_use_prev_reward': False, 
		'_time_major': False, 
		'use_attention': False, 
		'attention_num_transformer_units': 1, 
		'attention_dim': 64, 
		'attention_num_heads': 1, 
		'attention_head_dim': 32, 
		'attention_memory_inference': 50, 
		'attention_memory_training': 50, 
		'attention_position_wise_mlp_dim': 32, 
		'attention_init_gru_gate_bias': 2.0, 
		'attention_use_n_prev_actions': 0, 
		'attention_use_n_prev_rewards': 0, 
		'framestack': True, 
		'dim': 84, 
		'grayscale': False, 
		'zero_mean': True, 
		'custom_model': None, 
		'custom_model_config': {}, 
		'custom_action_dist': None, 
		'custom_preprocessor': None, 
		'lstm_use_prev_action_reward': -1}, 
	'optimizer': {}, 
	'explore': True, 
	'exploration_config': {'type': 'StochasticSampling'}, 
	'input_config': {}, 
	'actions_in_input_normalized': False, 
	'postprocess_inputs': False, 
	'shuffle_buffer_size': 0, 
	'output': None, 
	'output_config': {}, 
	'output_compress_columns': ['obs', 'new_obs'], 
	'output_max_file_size': 67108864, 
	'evaluation_interval': None, 
	'evaluation_duration': 10, 
	'evaluation_duration_unit': 'episodes', 
	'evaluation_sample_timeout_s': 180.0, 
	'evaluation_parallel_to_training': False, 
	'evaluation_config': {}, 
	'off_policy_estimation_methods': {}, 
	'evaluation_num_workers': 0, 
	'always_attach_evaluation_results': False, 
	'in_evaluation': False, 
	'sync_filters_on_rollout_workers_timeout_s': 60.0, 
	'keep_per_episode_custom_metrics': False, 
	'metrics_episode_collection_timeout_s': 60.0, 
	'metrics_num_episodes_for_smoothing': 100, 
	'min_time_s_per_iteration': None, 
	'min_train_timesteps_per_iteration': 0, 
	'min_sample_timesteps_per_iteration': 0, 
	'logger_creator': None, 
	'logger_config': None, 
	'log_level': 'DEBUG', 
	'log_sys_usage': True, 
	'fake_sampler': False, 
	'seed': None, 
	'_tf_policy_handles_more_than_one_loss': False, 
	'_disable_preprocessor_api': False, 
	'_disable_action_flattening': False, 
	'_disable_execution_plan_api': True, 
	'simple_optimizer': -1, 
	'monitor': -1, 
	'evaluation_num_episodes': -1, 
	'metrics_smoothing_episodes': -1, 
	'timesteps_per_iteration': -1, 
	'min_iter_time_s': -1, 
	'collect_metrics_timeout': -1, 
	'buffer_size': -1, 
	'prioritized_replay': -1, 
	'learning_starts': -1, 
	'replay_batch_size': -1, 
	'replay_sequence_length': None, 
	'prioritized_replay_alpha': -1, 
	'prioritized_replay_beta': -1, 
	'prioritized_replay_eps': -1, 
	'min_time_s_per_reporting': -1, 
	'min_train_timesteps_per_reporting': -1, 
	'min_sample_timesteps_per_reporting': -1, 
	'input_evaluation': -1, 
	'lr_schedule': None, 
	'use_critic': True, 
	'use_gae': True, 
	'kl_coeff': 0.2, 
	'sgd_minibatch_size': 128, 
	'num_sgd_iter': 30, 
	'shuffle_sequences': True, 
	'vf_loss_coeff': 1.0, 
	'entropy_coeff': 0.0, 
	'entropy_coeff_schedule': None, 
	'clip_param': 0.3, 
	'vf_clip_param': 10.0, 
	'grad_clip': None, 
	'kl_target': 0.01, 
	'vf_share_layers': -1, 
	'lambda': 1.0, 
	'input': 'sampler', 
	'multiagent': {
		'policies': {
			'agent_0': <ray.rllib.policy.policy.PolicySpec object at 0x7f09c1ba3490>, 
			'agent_1': <ray.rllib.policy.policy.PolicySpec object at 0x7f09c1ba3670>}, 
		'policy_map_capacity': 100, 
		'policy_map_cache': None, 
		'policy_mapping_fn': <function get_config.<locals>.policy_mapping_fn at 0x7f09c1bafdc0>, 
		'policies_to_train': None, 
		'observation_fn': None, 
		'replay_mode': 'independent', 
		'count_steps_by': 'env_steps'}, 
	'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 
	'create_env_on_driver': False, 
	'custom_eval_function': None, 
	'framework': 'tf', 
	'num_cpus_for_driver': 1}

